# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis_FIFA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10INR23cpPmOR1IyZNNldoMxMBGxAPmqo
"""

import numpy             as np
import pandas            as pd
import matplotlib.pyplot as plt
import seaborn           as sns
import plotly.graph_objs as go
import plotly.express    as px 
import nltk
import re
import string

from scipy.stats import norm
from wordcloud   import WordCloud, STOPWORDS

import warnings
warnings.filterwarnings("ignore")
from wordcloud import WordCloud,STOPWORDS
stopwords = set(STOPWORDS)

from textblob import TextBlob
import re
from collections import Counter

from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
from IPython.display import Markdown as md

!pip install snscrape

from google.colab import drive 
drive.mount('/content/gdrive')
data = pd.read_csv('gdrive/My Drive/Colab Notebooks/fifaworldcup2022.csv')

data.info()

data.head()

data.tail()

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
punc=string.punctuation
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')
nltk.download('punkt')
def clean_text(text):
    
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    
    #Removing stopwords
    text=" ".join([word for word in str(text).split() if word not in stop_words])
    
    #Stemming
    text = " ".join([stemmer.stem(word) for word in text.split()])
    
    #Lemmatization
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])
    
    return text

data['Text'] = data['Text'].apply(lambda x: clean_text(x))

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

data['Text']=data['Text'].apply(lambda x: remove_emoji(x))

def remove_punctuation(text):
    #Removes punctuation from a string using isalpha.
    new_text = ""
    for char in text:
        if char.isalpha() or char == " ":
            new_text += char
    return new_text
# Apply the remove_punctuation function to the sample string
data['Text']=data['Text'].apply(lambda x: remove_punctuation(x))

from IPython.utils import text
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Apply the function to the 'text' column of the dataframe
data['Text'] = data['Text'].apply(remove_punctuation)
print(text)

data.head()

data.tail()

def SentimentChecker (newTweets):
    newDF = pd.DataFrame(columns=['Text','Subjectivity','Polarity', 'Sentiment'])
    for i in newTweets: 
        blob = TextBlob(i) #Create Textblob object of a tweet
        pol = blob.sentiment.polarity
        sub = blob.sentiment.subjectivity
        
        if pol > 0 : #If polarity of tweets is positive then sent = positive
            sent = "Positive"
        elif pol < 0: #If polarity of tweet is negative then sent = negative
            sent = "Negative"
        else: #If polarity equals 0 then sent = neutral
            sent = "Neutral"
        
        #Create row for dataframe
        row = [i, sub, pol, sent] 
        #Add row to dataframe
        newDF.loc[len(newDF)] = row 
    return newDF

def JointPlotter(newDF):
    #Create jointplot for polarity and subjectivity of twitter tweets.
    sns.jointplot(data=newDF, x="Subjectivity", y="Polarity", hue="Sentiment")
    plt.show()
    
    sentimentCounts = pd.DataFrame(newDF["Sentiment"].value_counts())
    sentimentCounts = sentimentCounts.rename({'Sentiment': 'count'}, axis=1)
    sentimentCounts["Sentiment"] = sentimentCounts.index
    
    sns.barplot(data= sentimentCounts, x="Sentiment", y = "count") #Create barplot of sentiment counts
    
    plt.show()

def visualiser(subset):
    #Create frequency distribution for "subset"
    fdist = nltk.FreqDist()
    for i in subset["Text"]:
        i = nltk.word_tokenize(i)
        for j in i:
            fdist[j] +=1
    fdist.plot(30, cumulative=False)
    
    #Create word cloud for "subset"
    wordcloud = WordCloud(max_font_size=50, max_words = 100, background_color="white").generate_from_frequencies(fdist)
    plt.figure()
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

cleanTweets=data['Text'].head(1000)
SentimentDF = SentimentChecker(cleanTweets)
JointPlotter(SentimentDF)
    
#Call visualiser function to plot a wordcloud and a frequency distribution
visualiser(SentimentDF[SentimentDF["Sentiment"] == "Positive"])
visualiser(SentimentDF[SentimentDF["Sentiment"] == "Negative"])
visualiser(SentimentDF[SentimentDF["Sentiment"] == "Neutral"])